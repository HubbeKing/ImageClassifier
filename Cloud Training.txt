Training test, InceptionV3-based model:
    Amazon EC2 - m4.large (2 vCPU - 6.5 ECU) Amazon Linux Machine Learning AMI
        2 image classes
        2000 training samples (1000 per class)
        800 validation samples (400 per class)
            batch_size=10
            Epoch 1 - 2600s - acc: 0.87, val_acc: 0.97 (~45 mins)
            (~0.93s per image)

    Amazon EC2 - c4.xlarge (4 vCPU - 16 ECU) Amazon Linux Machine Learning AMI
        2 image classes
        2000 training samples (1000 per class)
        800 validation samples (400 per class)
            batch_size=10
            Epoch 1 - 1134s - acc: 0.5045, val_acc: 0.5000 (~19 mins)
            (~0.40s per image)

Amazon EC2:
    
    Data transfer costs should be very small, since our image size can be as small as 300x300
    Data transfer in is free.
    Data storage is $0.11/GB/month for an EBS volume    (ready to launch with instance)
                 is $0.05/GB/month for an EBS snapshot  (deep storage, needs a few minutes to create volume and attach to instance before launching)
        For 50GB of data - ~ $5/month ($2.5 if storing as snapshot instead of volume)

    Data transfer out costs ~ $0.15/GB after the first GB, but all we're transferring out is the model
    The model should be < 1 GB in size, so no charge for data transfer.

    
    Compute costs (Estimate at least a few days worth of computation for a classifier with lots of training data, for a good 50 epochs):
    (1 epoch = 1 pass over entire dataset)

    Nvidia K80 GPUs, EU Ireland
        p2.xlarge   - 1 GPU   $0.972/hour
        
        Single-GPU instance (p2.xlarge) EU Ireland - $0.972/hour
        
        Rough cost-estimate (4x24 = 96 hours):
            96 hours @ $0.972/hour = $93.31 = ~ €80
            Assuming non-ideal speedup of 6x and 10x for 8xlarge and 16xlarge instances:
                p2.8xlarge  - 16 * $7.776  = $124.416 = ~ €110
                p2.16xlarge - 10 * $15.552 = $155.520 = ~ €135
        
    Total training costs should ideally be ~ €100 per full round of training 
    Single round of training should be sufficient, provided the data set trained on is large enough
    (50 epochs at 2000 steps per epoch, plus 800 steps of validation)
    
    Next-gen p3 instances, EU Ireland (High-performance, optimized Tesla V100 GPUs)
        p3.2xlarge  - 1x Tesla V100 GPU - $3.305/hour

        96 hours on a p3.2xlarge (1x V100)
            96 * $3.305 = $317.28 = ~ €275
            
            
Microsoft Azure
    €170 free credit
    West US2:
        NC6 v2 instance
            6 CPU cores, 112GiB RAM, 336GiB storage, 1x P100 GPU
            €0.759/hour
            48 hours:  ~ €37
            96 hours:  ~ €73
            144 hours: ~ €109
            336 hours: ~ €255
        NC6 v1, same specs/price, 1xK80 GPU   
    